---
Title: Exercise 16: Enumerate HTTP Directories with Nmap NSE
Objective: Learn to use the Nmap Scripting Engine (NSE) specifically for web server reconnaissance, employing scripts like `http-enum` to discover common web directories, files, and applications on a target host.
Estimated Time: 20-40 minutes
Tools Needed:
*   Nmap (Network Mapper) - Download from [nmap.org](https://nmap.org/download.html)
Setup Instructions:
1.  Ensure Nmap is installed and accessible from your terminal.
2.  Identify a target IP address or hostname that is running a web server (e.g., has port 80 or 443 open). `scanme.nmap.org` has a web server.
---

## Steps

1.  **Identify Web Servers**
    *   First, perform a basic scan to confirm if the target has an HTTP/HTTPS service running:
        ```bash
        nmap -p 80,443 [TargetIP]
        ```
    *   *Observation:* Confirm that port 80 or 443 is reported as `open`.

2.  **Run the `http-enum` NSE Script**
    *   Open a terminal.
    *   Run the following command, replacing `[TargetIP]` with your target:
        ```bash
        nmap -p 80,443 --script http-enum [TargetIP]
        ```
    *   *Explanation:* The `http-enum` script attempts to discover web applications, directories, and files by brute-forcing common names and checking for responses. It's useful for finding hidden content or administrative interfaces.

3.  **Analyze the Output**
    *   Review the output generated by the `http-enum` script.
    *   *Observation:* Look for discovered paths, especially those that might indicate administrative panels (e.g., `/admin`, `/login`), version control systems (e.g., `.git`), or sensitive files (e.g., `robots.txt`, `sitemap.xml`). The script often indicates if a path is allowed or disallowed by `robots.txt`.

4.  **Explore Other HTTP-Related Scripts (Optional)**
    *   Nmap has many other useful HTTP-related scripts, such as:
        *   `http-title`: Displays the title of the web page.
        *   `http-headers`: Shows HTTP headers.
        *   `http-security-headers`: Checks for common security headers.
        *   `http-robots.txt`: Parses `robots.txt` for disallowed entries.
    *   You can run multiple scripts by separating them with commas:
        ```bash
        nmap -p 80,443 --script "http-enum,http-title,http-headers" [TargetIP]
        ```
    *   *Observation:* Observe the additional information provided by these scripts.

## Expected Output

```
Starting Nmap 7.94 ( https://nmap.org ) at 2024-02-24 16:00 EST
Nmap scan report for scanme.nmap.org (45.33.32.156)
Host is up (0.050s latency).
PORT      STATE    SERVICE
80/tcp    open     http
| http-enum: 
|   /robots.txt
|   /sitemap.xml
|_  /admin/ (Disallowed by robots.txt)
|_http-title: Go ahead and ScanMe!
443/tcp   open     https
|_http-title: Go ahead and ScanMe!

Nmap done: 1 IP address (1 host up) scanned in 15.20 seconds
```
![Expected output](images/exercise-16-output.png)

## Reflection Questions

1.  How can the information provided by `http-enum` assist in identifying potential attack vectors or sensitive information on a web server?
2.  What is the significance of discovering directories listed as "Disallowed by robots.txt"?
3.  How does combining `http-enum` with other Nmap HTTP scripts provide a more comprehensive view of a web server's attack surface?

## Next Steps

*   Proceed to the next tool set: Python Exercises.
*   Further reading: [Nmap Script Reference - http-enum](https://nmap.org/nsedoc/scripts/http-enum.html)

## Hints / Troubleshooting

*   **Target Selection:** Ensure your target actually has a web server running on the specified ports.
*   **Time:** Running `http-enum` can take longer than simple port scans, as it performs numerous HTTP requests.
*   **Firewalls/WAFs:** Web Application Firewalls (WAFs) can detect and block `http-enum` scans, especially if they are configured to look for directory brute-forcing.
*   **False Positives:** While generally accurate, some paths discovered might be false positives or lead to empty directories. Manual verification is always recommended.
